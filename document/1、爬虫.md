### 爬虫基本概念

##### 爬虫设计的思路是：

	（1）给我一个url
	（2）模拟浏览器通过http协议访问url，获取到这个url的html代码
	（3）解析字符串（根据一定规则提取你所需要的数据）
##### GET和POST请求的区别

1.传参不同

get:将参数拼接在url后方

post:把提交的数据放置在是HTTP包的包体中

2.传输数据大小

get:一般浏览器有一定的限制   

post：无限制

3.安全性

post相对更安全

##### 请求头的说明

```
 浏览器端可以接受的媒体类型 ，MIME
Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,/;q=0.8
浏览器指定可压缩方式
Accept-Encoding:gzip, deflate, br
浏览器声明接受的语言  q为权重  (1最高)
Accept-Language:en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7
缓存相关
Cache-Control:max-age=0（默认浏览器关闭清楚）
连接方式，保持长连接
Connection: keep-alive 
会话相关
Cookie:。。。。。
主机
Host:www.baidu.com
浏览器将http请求自动升级为https
Upgrade-Insecure-Requests:1
向服务器发送浏览器的版本、系统、应用程序的信息。
User-Agent:。。。。
如果是ajax请求，一般都带这个
X-Requested-With: XMLHttpRequest
上一个页面，你从哪个页面过来的
	Referer: https://www.baidu.com/?tn=57095150_6_oem_dg

```

##### 常见的http状态码

```
常见的http状态码

100：继续 客户端应当继续发送请求。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。
101： 转换协议 在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。只有在切换新的协议更有好处的时候才应该采取类似措施。
102：继续处理 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。
200：请求成功 处理方式：获得响应的内容，进行处理
201：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到 处理方式：爬虫中不会遇到
202：请求被接受，但处理尚未完成 处理方式：阻塞等待
204：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。 处理方式：丢弃
300：该状态码不被HTTP/1.0的应用程序直接使用， 只是作为3XX类型回应的默认解释。存在多个可用的被请求资源。 处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃
301：请求到的资源都会分配一个永久的URL，这样就可以在将来通过该URL来访问此资源 处理方式：重定向到分配的URL
302：请求到的资源在一个不同的URL处临时保存 处理方式：重定向到临时的URL
304：请求的资源未更新 处理方式：丢弃，使用本地缓存文件
400：非法请求 处理方式：丢弃
401：未授权 处理方式：丢弃
403：禁止 处理方式：丢弃
404：没有找到 处理方式：丢弃
500：服务器内部错误 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器端的源代码出现错误时出现。
501：服务器无法识别 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。
502：错误网关 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。
503：服务出错 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。

```

#### urllib

##### urllib.request  

1.urlopen(url,data) 用来打开一个网页或者url对象 ,

参数1:url    参数2:data  带参数2的话是post请求

2.urlretrieve(url, filename)  下载图片

参数1:url  参数2：保存的内容路径

3.Request构建请求对象

```
request = urllib.request.Request(url=url, headers=headers, data=data)
response = urllib.request.urlopen(request)
```

##### urllib.parse

1.urlencode()  ,如果是post请求,则要将data进行编码

2.quote()  把get请求的url字符串编译成url可识别的url

  unquote()  解码

##### response对象

```
字符串类型和字节类型
		字符串-》字节：编码  encode()
		字节-》字符串：解码  decode()
	readline():读取一行
	readlines()：读取全部，返回一个列表
	getcode()：状态码
	geturl()：获取url
	getheaders():响应头信息，列表里面有元组
```

##### get请求

核心是参数传递方式

```
data = {
    'start': (page -1) * 20,
    'limit': '20'
}
# 将data拼接为get请求的查询字符串
data = parse.urlencode(data)
url = url + data
```

post请求

```
data = {
'cname': '深圳',
'pid': '',
'pageIndex':'2',
'pageSize': '10'
}

# 转换格式
data = parse.urlencode(data).encode('utf-8')
```

##### 代理

http://www.kuaidaili.com/  快代理网址
http://www.xicidaili.com/  西刺代理网址

【注】访问的网址是http时候，你用http代理服务器，访问https的时候，你用https代理服务器

##### Handler处理器

因为request无法携带请求头,所以引入了Handler处理器

###### 1.简单Handler使用:

```
# 首先创建一个handler对象
handler = urllib.request.HTTPHandler()
# 创建opener对象
opener = urllib.request.build_opener(handler)
response = opener.open(request)
print(response.read().decode('utf-8'))
```

###### 2.代理handler使用:

```
# 配置代理
handler = urllib.request.ProxyHandler({'http': '101.205.84.107:808'})
# 创建opener
opener = urllib.request.build_opener(handler)
# 发送请求
response = opener.open(request)
print(response.read().decode('utf-8'))
```

###### 3.携带cookie使用：

```
import urllib.request
import http.cookiejar
import urllib.parse

# 创建一个cookiejar对象
cookie = http.cookiejar.CookieJar()
# 根据cookiejar对象创建handler对象
handler = urllib.request.HTTPCookieProcessor(cookie)
# 根据handler对象创建一个opener对象
opener = urllib.request.build_opener(handler)

response = opener.open(request)
# 此时的opener已经携带了cookie  下次再打开request时自动携带
print(response.read().decode('utf-8'))
```

##### 学习参考网站api文档，进行url请求

```python
import random
import string
import time
import hashlib
from urllib import parse, request
def send_sms():
    # 请求的url
    api = 'https://api.netease.im/sms/sendcode.action'

    # 准备headers中的参数
    # 网易云信平台分配的appkey
    AppKey = '097c1bd21e9e24279517ddbe07cf8a84'
    # 最大128个字符的随机字符串
    Nonce = ''.join(random.sample(string.ascii_letters + string.digits, 20))
    # 当前UTC时间戳从1970年1月1日0点0 分0 秒开始到现在的秒数(String)
    CurTime = str(int(time.time()))
    #appsecret
    AppSecret = '12b619be2374'
    # 校验码
    Sumstr = AppSecret + Nonce + CurTime
    sha = hashlib.sha1(Sumstr.encode())
    CheckSum = sha.hexdigest()
    # 另一种使用hash的用法。
    hash = hashlib.sha1()
    hash.update(Sumstr)
    hash.hexdigest()

    # 设置头
    headers = {
        'AppKey': AppKey,
        'Nonce' : Nonce,
        'CurTime': CurTime,
        'CheckSum': CheckSum,
        'Content-Type': 'application/x-www-form-urlencoded',
        'charset': 'utf-8'
    }

    # 准备要发送的数据，必填的只有一个手机号
    data = {
        'mobile': '18676689715',
        'templateid': '4032501'
    }
    # 把字典解析成url中传参的格式，即：mobile='18676689715'
    data = parse.urlencode(data).encode()
    # 创建request实例对象
    r = request.Request(url=api, headers=headers, data=data)
    req = request.urlopen(r)
    response = req.read()
    print(response)

if __name__ == '__main__':
    send_sms()
```

### XPATH

##### xpath介绍

xpath的学习网址

http://www.w3school.com.cn/xpath/

安装:

pip install lxml  -i https://pypi.douban.com/simple

```
	//  从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置
	.   选取当前节点
	@   选取属性

注意：
索引定位：下标从1开始
	查找id是maincontent的div下面的h1节点
	//div[@id="maincontent"]/h1

	//div[@class="head_wrapper"]/div[@id="u"]/a[1]
逻辑运算
	//div[@id="head" and @class="s_down"]
模糊匹配
	查找所有的div，id中有he的div
	//div[contains(@id, "he")]
	查找所有的div，id中以he开头的div
	//div[starts-with(@id, "he")]
	查找所有的div，id中以he结尾的div
	//div[ends-with(@id, "he")]
取文本
	//div[@class="head_wrapper"]/div[@id="u"]/a[1]/text()
	//div[@class="head_wrapper"]/div[@id="u"]/a[1]
	obj.text   将内容获取到
取属性
	//div[@class="head_wrapper"]/div[@id="u"]/a[1]/@href

	
```

##### 使用xpath

```
from lxml import etree
html_etree = etree.parse('可以获取本地html文件')     
html_etree = etree.HTML('网上获取的html字符串(也可以是字节类型)')
html_etree.xpath('xpath路径')
返回的是一个列表
```

### BS4

和xpath一样，是 用来检索html标签的库

安装:

pip install bs4

BS4的使用:

！！！选择后返回的一定是一个列表！！！！

```python
获取内容：(string,get_text)   带()不带都行
1. 字符串搜索
	print(soup.find('a'))   # 找到第一个a标签
	print(soup.find_all('a'))   # 找到所有的a标签
2. 正则搜索
    import re	# 找到所有以b开头的标签
	for tag in soup.find_all(re.compile("^b")):
    	print(tag.name)
3.  多个条件搜索
	传列表		# 找到a，或者b的标签
  	ret = soup.find_all(["a", "b"])
	print(ret)
4.  key-value查找
	# keyword参数
	ret = soup.find_all(id = 'link2' )   # 找到所有id为link2的标签
	print(ret)
	ret = soup.find_all(classe_='sister')	 # 找到所有class带有link2的标签
	print(ret)
4. css选择器查找
	print(soup.select('title'))			#  找到title的标签
	print(soup.select('.sister'))		# 找到class包含sister的标签
	print(soup.select('head > title'))	# 找到 head标签下的title标签
```

实例使用:

```python
from bs4 import BeautifulSoup

request = urllib.request.Request(url=url, headers=headers)
response = urllib.request.urlopen(request)

# 创建对象的时候，一定要加lxml
soup = BeautifulSoup(response.read(), 'lxml')
jobname = soup.select('.zwmc > div > a')[0].string

```

### json对象

json对象:  最外层是{}的形式

json数组:  最外层是[]的形式

json/dump       和   jsons/dumps

其实文件就不加s   

不是文件就加s

```python
load和loads
    # 读取文件中json形式的字符串元素 转化成python类型
    obj = json.load(open('../file/book.json', 'r', encoding='utf-8'))
    print(type(obj), obj)    # 输出内容同下

    # 将json格式字符串转化为对象
    with open('../file/book.json', 'r', encoding='utf-8') as fp:
        json_string = fp.read()
    obj = json.loads(json_string)
    print(type(obj), obj)   # # 输出内容同上
dump和dumps
	# 实现python类型转化为json字符串，返回一个str对象 把一个Python对象编码转换成Json字符串
	str = '''
    {"has_more": false, "message": "success", "data": [{"single_mode": true, "abstract": 	"\u8c22\u8c22\u5927\u5bb6\u559c\u6b22\u6bcf\u65e5\u64b8"}]}
    '''
    obj = json.dumps(str, ensure_ascii=False)
    print(type(obj), obj)   # 输出字符串   json格式的字符串

    # 使用dump把字符串写入到文件中，将Python内置类型序列化为json对象后写入文件
    dictStr = {"city": "北京", "name": "大刘",'info':'\u8c22\u8c22\u5927'}
    json.dump(dictStr, open('dump.json', 'w', encoding='utf-8'), ensure_ascii=False)

```

##### jsonpath

JsonPath 对于 JSON 来说，相当于 XPATH 对于 XML

![jsonpath](C:\Users\sdsd\Desktop\笔记\img\jsonpath.png)



### requests库

| r.text        | 网页源代码(字符串) |
| ------------- | ---------- |
| r.encoding    | 网页编码方式     |
| r.url         | 网页的urll    |
| r.content     | 响应的字节类型    |
| r.status_code | 响应状态码      |
| r.headers     | 响应的头部信息    |

##### get请求

1.不带参数

r= requsets.get(url=url)

r.text() 			# 网页源代码

r.encodeing 		#  网页编码方式（可设置）

2.带参数

data = {

​	'wd': '日本'

}

r = requset.get(url=url ,  params=data,  headers=headers)

r.text

##### post请求

data = {

​	'kw': 'hello'

}

r = requests.post(url=url ,  data=data)

​        自动 将返回的json解码。

print(r.json())

##### 使用代理

其实requests全部都是字典传值,对应传参即可

proxy = {

​	'https': '114.237.147.155:9999'

}

只需要传入proxies参数即可。

r = requests.get(url=url, params=data, headers=headers, proxies=proxy)

##### 保存cookie会话

```python
# session自动记录cookie
session = requests.Session()
# 此时session创建后用post打开的网页自动携带了cookie相关信息
session.post(url=login_url, data=data, headers=headers)
r = session.get(url=url, headers=headers)
print(r.text)
```

