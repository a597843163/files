### 线程锁和队列锁

本质其实就是join方法

线程.join       等待 线程执行结束后解锁

队列.join        等待  队列清空任务后解锁    ———— >>>   标志:  q.task_done()

代码区别：

```python
# 线程锁和队列锁。
import threading
import queue

# 创建queue
q = queue.Queue(10)

# 加一个全局标志，让线程能判断何时退出。
exit_flag = False
def run(self):
  # 不停的从队列中取数据。
  # 死循环
  while True:
    # 增加退出判断
    global  exit_flag
    if exit_flag:
      break
      # 取数据，并设置不等待
      try:
        value = self.q.get(block=False)
        # 取任务完成, 取出后向队列发送一个信号。队列的join方法以此来判断队列是否为空。如果取之后不调用task_done，则join将挂死。
        q.task_done()
        except:
          pass
def main():
    # 往队列中放10个数据
    print('主线程开始')
    for i in range(10):
        q.put(i)

    # 启动多线程
    mythread = MyThread(q)
    mythread.start()
    # 等待线程执行完成后设置退出标志， 这就是所谓的线程锁。
    # mythread.join()
    # 设置退出标志为True，上面的线程没跑完，下面的设置代码永远不会做。而线程中线程要退出又依赖于退出标志。

    # 设置队列锁，队列为空才会继续做。这样当线程把队列清空之后，队列锁就释放了。
    q.join()
    global  exit_flag
    exit_flag = True
    print('主线程结束')
```

### 多线程爬虫(重点)

图解:![多线程爬虫实现原理](C:\Users\sdsd\Desktop\笔记\img\多线程爬虫实现原理.png)

基本思路：

```python
需要设置两个队列，和解析线程的退出标志
1. 创建爬虫线程，并且启动。
2. 创建解析线程，启动，并保存数据到本地文件中。
3. 队列锁，保证任务执行结束。
4. 设置退出标志为True，退出线程，工作结束。
5. 关闭文件
```

代码:

```python
import json
from urllib.error import URLError
import requests
import queue
import threading
import random
import time
from lxml import etree

# 创建2个队列
# 创建爬虫队列
spider_queue = queue.Queue()

# 创建解析队列
parse_queue = queue.Queue()

# 设置解析线程退出标志
parse_exit_flag = False

# 基础url
base_url = 'https://www.qiushibaike.com/8hr/page/%d/'

# 文件操作锁
lock = threading.Lock()

# headers 列表
UserAgent_List = [
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2224.3 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 4.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2309.372 Safari/537.36",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2117.157 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1866.237 Safari/537.36",
]


class SpiderThread(threading.Thread):
    def __init__(self, id, s_queue, p_queue, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.id = id
        self.spider_queue = s_queue
        self.parse_queue = p_queue

    def run(self):

        print('%d线程开始执行' % self.id)
        # 循环从爬虫队列中取数据。
        while True:
            # 当队列为空时，终止循环
            if self.spider_queue.empty():
                break
            # 取数据
            page = self.spider_queue.get()

            # 拼接url
            url = base_url % page
            # 避免因为网络不好数据无法取出，尝试四次。
            times = 4
            while times > 0:
                try:
                    response = requests.get(url=url, headers={'User-Agent': random.choice(UserAgent_List)})
                    # 把数据放入解析队列
                    self.parse_queue.put(response.text)
                    # 爬的时候不要太快，睡1秒
                    time.sleep(1)
                    print('%d线程成功抓取第%d页的数据' % (self.id, page))
                    # 事情做完才 通知队列数据取出
                    self.spider_queue.task_done()
                    # 成功一次就break
                    break
                except URLError as e:
                    print('网络错误')
                finally:
                    times -= 1


class ParseThread(threading.Thread):
    def __init__(self, id, p_queue, fp, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.id = id
        self.parse_queue = p_queue
        self.fp = fp

    def run(self):
        global parse_exit_flag
        # 循环解析队列中的数据
        while True:
            # 当解析退出标志为True时，退出循环
            if parse_exit_flag:
                break
            # 解析有可能出错。
            try:
                # 从解析队列中取数据, 注意设置成False
                data = self.parse_queue.get(block=False)
                # 解析数据，封装成一个函数
                self.parse(data)
                print('%d线程解析数据成功' % (self.id))
                # 告诉队列取完了数据
                self.parse_queue.task_done()


            except Exception as e:
                pass
                # print(e, '解析错误')

    def parse(self, data):
        global parse_num
        # 数据保存到一个文件中。
        # 先解析数据
        # 创建etree对象
        html = etree.HTML(data)
        # 找出页面上所有的段子div
        div_list = html.xpath('//div[contains(@id,"qiushi_tag_")]')
        # 对div_list中的每一个div进行数据解析
        results = []
        for div in div_list:
            # 头像的url
            head_shot = div.xpath('.//img/@src')[0]
            # 作者名字
            name = div.xpath('.//h2')[0].text
            # 内容
            content = div.xpath('.//span')[0].text.strip('\n')
            # 保存到一个字典中
            item = {
                'head_shot': head_shot,
                'name': name,
                'content': content
            }
            results.append(item)
            # 保存到文件中，并且加锁
        json_result = {
            'msg': 'ok',
            'status': 200,
            'data': results
        }
        with lock:
            self.fp.write(json.dumps(results, ensure_ascii=False) + '\n')


def main():
    print('--------------任务开始')
    # 假设爬10页数据。往爬虫队列中写入10个数
    for page in range(1,11):
        spider_queue.put(page)

    # 1. 创建爬虫进程，并启动
    # 创建3个爬虫线程
    for i in range(3):
        SpiderThread(i, spider_queue, parse_queue).start()

    # 2. 创建解析进程并启动。
    # 创建3个解析线程，并保存解析的内容
    # 数据保存到一个文件中
    fp = open('./qiubai.txt', 'w', encoding='utf-8')
    for i in range(3):
        ParseThread(i, parse_queue, fp).start()

    # 3. 队列锁，保证任务执行结束。
    spider_queue.join()
    parse_queue.join()

    # 4. 设置关闭退出标志。
    global parse_exit_flag
    parse_exit_flag = True

    # 关闭文件
    fp.close()

    print('--------------任务完成')
if __name__ == '__main__':
    main()
```

### selenium

用于处理js加载的页面,可以模仿浏览器访问页面获得数据

安装:

pip install selenium

selenium自己不带浏览器，需要其他辅助浏览器

如：

##### 	Phtontomjs

​	下载安装参考day08资料

​	使用时需设置path路径   phtontomjs的安装路径

​	PhantomJS 是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效。

selenium命令操作:

```python
# 键盘命令操作
send_keys(Keys.BACK_SPACE)                   	删除键
send_keys(Keys.SPACE)                           空格键
send_keys(Keys.TAB)                             制表键
send_keys(Keys.ESPACE)                          回退键
send_keys(Keys.ENTER)                           回车键
send_keys(Keys.CONTROL,'a')                     全选
send_keys(Keys.CONTROL,'c')                     复制
send_keys(Keys.CONTROL,'x')                     剪切
send_keys(Keys.CONTROL,'v')                     粘贴	
send_keys(Keys.F1)                              F1

# 基本使用

# 创建对象
form selenium import webdriver
# 创建以模拟谷歌浏览器的对象
driver = webdriver.Chorms()
# 打开网页
driver.get('https://movie.douban.com/')
# 对当前页面截屏
driver.save_screenshot('douban2.png')		
# 以元素属性带有id为kw为查询条件
my_input = driver.find_element_by_id('kw')
# 用xpath来查询元素
my_input = driver.find_elements_by_xpath('//form[@id="form"]/span[1]/input')[0]
# 用css选择器来选择元素
my_input = driver.find_elements_by_css_selector('#kw')[0]
# 输入内容
my_input.send_keys('美女')
# 点击元素
my_button.click()
# 模拟浏览器后退操作
driver.back()
# 前进
driver.forward()
# 找到可点击连接内容是'设置'的链接  并且点击
driver.find_elements_by_link_text('设置')[0].click()
# 处理弹出的警告页面   确定accept() 和 取消dismiss()
driver.switch_to_alert().accept()   # 确定
driver.switch_to.alert.dismiss()	# 取消

# 模拟滚动
# 滚动到距离顶部10000的位置(其实就是到底)
js = 'document.documentElement.scrollTop=10000'
driver.execute_script(js)
# 自己写js设置页面
# 给搜索输入框标红的javascript脚本

js = "var q=document.getElementById(\"kw\");q.style.border=\"2px solid red\";"
# 调用给搜索输入框标红js脚本
driver.execute_script(js)

#  窗口跳转
handles = driver.window_handles
driver.switch_to.window(handles[0])
driver.switch_to.window(handles[1])
```

### tesseract

说明:图片处理工具   把图片变成字符串

环境配置:

需要把tesseract和训练数据加入到系统的环境变量里面去

```python
import pytesseract
from PIL import Image
from  time import sleep

image = Image.open('../img/captcha.jpg')

# 对图片做一个灰度处理
image = image.convert('L')
data = image.load()
w, h = image.size
for i in range(w):
    for j in range(h):
      	#  大于50的颜色  设为全白 
        if data[i, j] > 50:
            data[i, j] = 255
        else:
            # 设成全黑
            data[i, j] = 0
# image.show()
image.save('../img/self.jpg')
# 执行tesseract 语句  
result = pytesseract.image_to_string(image)
print(result)

```

